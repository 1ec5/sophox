import re
from collections import defaultdict

from pywikiapi import Site, AttrDict
from pywikibot import Site as PbSite

from .CacheInstances import Caches
from .ItemFromWiki import ItemFromWiki
from .UploadItem import UploadItem
from .Properties import P_KEY_ID, P_TAG_ID
from .utils import get_entities, list_to_dict_of_lists, sitelink_normalizer, strid_from_item


class Processor:

    def __init__(self, opts, caches: Caches, site: Site, pb_site: PbSite) -> None:
        self.opts = AttrDict({
            'throw': True,
            'props': False,
            'ignore_qid': False,
            'overwrite_user_labels_en': True,
            'overwrite_user_labels': False,
            'overwrite_user_descriptions': False,
            'overwrite_user_claims': False,
            **(opts or {})
        })
        self.caches = caches
        self.site = site
        self.pb_site = pb_site

        wiki_items = (
                []
                + self.caches.keydescription.get()
                + self.caches.tagdescription.get()
        )

        # For each variant of str_id, count how many variants there are.
        # Ideally should be 1 for each, but if something defines two variants,
        # e.g. "Key:blah_blah" and "Key:blah blah",
        # each of the values will have 2 or more
        self.str_id_variants_count = defaultdict(int)
        for s in [
            set(lst)
            for k, lst in list_to_dict_of_lists(
                set([v.str_id for v in self.caches.descriptionParsed.get() if v.str_id] +
                [v for v in self.caches.itemKeysByStrid.get().keys()]),
                lambda v: sitelink_normalizer(v)).items()
        ]:
            for v in s:
                self.str_id_variants_count[v] = len(s)
            if len(s) > 1:
                print('Ambiguous entries: "' + '", "'.join(s) + '"')

        existing_items_strids = set(self.caches.itemKeysByStrid.get().keys())

        self.new_items_strids = set([v.str_id for v in wiki_items
                                     # if 'direction' in v.str_id  #DEBUG
                                     ]) - existing_items_strids

        self.autogenerated_keys = set()
        self.all_items_by_strid = {
            **self.items_by_strid(P_KEY_ID),
            **self.items_by_strid(P_TAG_ID),
        }

        self.wiki_items_by_id = list_to_dict_of_lists(wiki_items, lambda v: v.str_id)

    def items_by_strid(self, prop, fix_multiple=False):
        result = {}
        for item in self.caches.data_items.get():
            qid = item.id
            try:
                values = prop.get_claim_value(item, allow_multiple=True)
                if not values or len(values) == 0:
                    continue
                if len(values) == 1:
                    result[values[0]] = item
                    continue
                if not fix_multiple:
                    raise ValueError('Found multiple key ids ')
                if len(set(values)) > 1:
                    raise ValueError('Found multiple different keys')

                print(f'Removing multiple duplicate keys from { self.caches.qitem(qid)}...')
                raise ValueError('Not implemented')
            except ValueError as err:
                print(f'Error parsing key id from { self.caches.qitem(qid)}: {err}')

        return result

    def run(self, mode):
        if mode == 'new':
            items = self.new_items_strids
        elif mode == 'old':
            items = self.caches.itemKeysByStrid.get()
        elif mode == 'autogen_keys':
            items = self.autogenerated_keys
        elif mode == 'taginfo_keys':
            items = self.get_taginfo_keys()
        elif mode == 'items':
            items = self.caches.data_items.get()
        else:
            items = [mode] if type(mode) == str else mode
            mode = f'single object - {mode}'

        print(f'********** Running in {mode} mode')
        for obj in items:
            try:
                self.do_item(obj)
            except Exception as err:
                print(f'Crashed while processing "{obj}": {err}')
                if self.opts['throw']:
                    raise

    def do_item(self, obj):
        if not obj:
            return
        item = None
        wiki_pages = None

        if type(obj) == str:
            strid = obj
        else:
            item = obj
            strid = strid_from_item(item)
            if not strid:
                print(f'Skipping item without strid {item.id}')
                return
        if strid in self.str_id_variants_count and self.str_id_variants_count[strid] > 1:
            print(f'Skipping ambiguous "{strid}"')
            return
        if strid in self.wiki_items_by_id:
            wiki_pages = self.wiki_items_by_id[strid]
        if strid in self.all_items_by_strid:
            item2 = self.all_items_by_strid[strid]
            if not item:
                item = item2
            elif item2 and item != item2:
                print(f'Skipping {strid} because it matched {item.id} and {item2.id}')
                return

        change, sitelink = self.do_item_run(strid, item, wiki_pages, True)
        if change:
            if wiki_pages:
                unparsed = self.caches.description.get_new_pages([p.full_title for p in wiki_pages])
                wiki_pages = self.caches.descriptionParsed.parse_manual(unparsed)
            if item:
                item = get_entities(self.site, ids=item.id)
            else:
                item = get_entities(self.site, titles=sitelink)
            if item:
                item = AttrDict(item)
            self.do_item_run(strid, item, wiki_pages, False)

    def do_item_run(self, strid, item, wiki_pages, dry_run):
        parsed_item = ItemFromWiki(self.caches, strid, wiki_pages)
        parsed_item.run()
        if not parsed_item.ok:
            parsed_item.print_messages()
            return None, None

        status_id = strid if strid else ''
        if item:
            status_id += ' ' + self.caches.qitem(item.id)

        uploader = UploadItem(self.caches, self.autogenerated_keys, self.pb_site, strid, item, parsed_item.editData,
                              parsed_item.claims, self.opts, dry_run)
        uploader.prepare_upload()
        if dry_run:
            if not uploader.needs_changes:
                # Do not print messages if we will repeat the process anyway
                parsed_item.print_messages()
                uploader.print_messages()
        else:
            if uploader.needs_changes:
                print(f'==== Updating {status_id} ====')
                parsed_item.print_messages()
                uploader.print_messages()
                uploader.upload_item_updates()
                uploader.print_messages()
                print(f'==== Done updating {status_id} ====')
            else:
                parsed_item.print_messages()
                uploader.print_messages()

        return uploader.needs_changes, parsed_item.sitelink

    def get_nonbot_editors(self, qid):
        resp = self.site('query', prop='contributors', pclimit='max', titles='Item:' + qid)
        contributors = [v.name for v in resp.query.pages[0].contributors]
        return ', '.join([c for c in contributors if c != 'Yurikbot']) if contributors != ['Yurikbot'] else False

    def get_taginfo_keys(self):
        re_key = re.compile(r'^[a-z0-9]+([-:_.][a-z0-9]+)*$')
        known_keys = self.caches.itemKeysByStrid.get()
        for item in self.caches.taginfo.get()['data']:
            key = item['key']
            count_all = item['count_all']
            if key in known_keys or count_all < 500 or (count_all < 1000 and not re_key.match(key)):
                continue

            yield key
